# 百面机器学习笔记

## 第一章  特征工程 Feature Engineering

## 特征归一化

> 数据和特征往往决定了结果的上限，而模型，算法的选择以及优化是在逐步接近这个上限
> 特征归一化，对于使用梯度下降优化的算法，如Liner regression, Logistic regression,SVM,NN等，通常需要归一化。
>> 如果不进行归一化,数值数量级大的特征的更新更快。而对于决策树模型，结点分裂基于数据集D关于特征x的信息增益比，其余是否归一化无关

## 类别型特征

> 序号编码 Ordinal encoding 常用于处理类别间具有大小关系的数据
> 独热编码 One-hot encoding 同序号编码相异，常用于处理类别间不具有大小关系的数据
>> 需要注意：

1. 采用稀疏向量来表示，以节省空间
2. 高特征维度可能会带来一些问题：
    * 高维空间中的距离难以度量
    * 参数随着维度的增加而增加，容易引起过拟合
    * 维度很高但只有部分真正对目标有意义

> 二进制编码 Binary encoding 类别id二进制化
> Helmert Contrast, Sum Constrast, Polynomial Contrast, Backward Difference Contrast

## 高维特征组合的处理

> 为提高复杂关系的你和能力，在特征工程中常会把一阶特征两两组合，构成高阶特征。
> 直接组合对于实际问题中特征特别多的情况，最后的组合空间会爆炸。一般先把原始特征用较低维的向量表示，再进行组合
> 简单的两两组合并不一定会有意义，还会引起参数过多过拟合的问题

## 组合特征

> 构造决策树，以决策树的路径作为一种特征组合

## 文本表示模型

1. 词袋模型 Bag of Words 和 TF-IDF Term Frequency-Inverse Document Frequency

    > 忽略词出现的顺序，将整段文本以单词切分开，用一个长向量来表示，每一个维度代表一个单词
    > $$ TF-IDF(t,d) = TF(t,d) \times IDF(t) $$
    > $$ IDF(t) = \log(\frac{{文章总数}}{ {包含单词t的文章总数+1}}) $$
    > 一个词在很多文本中出现，说明只是一个常用词，对分析的意义并不大
    >> 词的组合不同可能导致意义的变化，所以引入了N-gram模型，即N个连续词组成一个词组
    >> 单词有多种词性，而意思和词干一致，所以一般会进行词干抽取(Word Stemming)

2. Topic Model

3. Word Embedding

> 词嵌入 将每个单词映射至低维空间上的一个稠密向量，对于一篇有N个词的文章，可以用一个 $ N \times K $的矩阵表述

## Word2Vec

> 浅层神经网络模型，有两种结构

1. CBOW Continues Bag of Words 根据上下文的词语来预测当前词的生成概率
2. Skip-gram 根据当前词来预测上下文的生成概率

* 输入层均采用热度编码
* 由于SoftMax 函数有一个归一化项， 所以迭代时需要遍历所有的词汇表中的单词，使得迭代很慢，基于词有了 Hierarchical Softmax 和 Negative Sampling 两种改进

> LDA 利用文档中单词的 **共现** 关系来对单词按主题聚类，所以词向量中融入了上下文共现的特征，即两个词的w2v向量的相似度较大时，经常会在同样的上下文中出现

## 数据扩充 Data Augmentation

> 迁移学习 Transfer Learning， GAN，图像处理， 上采样， DA
>问题，数据集小，提供的信息少

1. 简化模型 非线性变线性， 加正则项，集成学习，Dropout
2. 数据扩充 基于先验知识，在保持数据的特定信息的前提下，对原始数据进行适当的变换

    * 图像的部分仿射变换
    * 图像加噪
    * RGB空间的PAC后的微小偏移
